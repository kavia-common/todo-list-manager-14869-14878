{"is_source_file": true, "format": "TypeScript", "description": "This file defines a 'robots' function that returns a configuration object for web crawlers, specifying rules and optionally a sitemap, likely used for setting up robots.txt directives in a Next.js project.", "external_files": ["next"], "external_methods": [], "published": ["default"], "classes": [], "methods": [{"name": "export default function robots(): MetadataRoute.Robots { robots", "description": "Returns a robots configuration object with rules allowing all user agents access and no sitemap specified.", "scope": "", "scopeKind": ""}], "calls": ["MetadataRoute.Robots"], "search-terms": ["robots", "MetadataRoute", "next", "robots.txt", "web_crawlers", "Next.js", "configuration"], "state": 2, "file_id": 17, "knowledge_revision": 57, "git_revision": "f61587d0470d9014d76e53294004e3ec0fed8aa0", "ctags": [{"_type": "tag", "name": "robots", "path": "/home/kavia/workspace/code-generation/todo-list-manager-14869-14878/web_frontend/src/app/robots.ts", "pattern": "/^export default function robots(): MetadataRoute.Robots {$/", "language": "TypeScript", "kind": "function"}], "hash": "0fc1f3157e46b00d83b7f3ad250bda64", "format-version": 4, "code-base-name": "web_frontend", "filename": "web_frontend/src/app/robots.ts", "revision_history": [{"57": "f61587d0470d9014d76e53294004e3ec0fed8aa0"}]}